{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ils en ont 92 features pour entraîner groupe 52"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries for data manipulation\n",
    "import pandas as pd\n",
    "from summarytools import dfSummary\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from icecream import ic\n",
    "#ic(my_var) : fait un joli print direct\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# import latexify\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import copy as cp\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "\n",
    "# Import libraries for building linear regression model\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Import the required function\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import library for preparing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import library for data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# Plot q-q plot of residuals\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "\n",
    "## R2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "####################\n",
    "# Fonctions : \n",
    "\n",
    "\n",
    "\n",
    "def corr_plot(df, upper_tri=None, threshold=None):\n",
    "    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
    "\n",
    "    corr = df.corr()\n",
    "    mask = None\n",
    "\n",
    "    if threshold != None:\n",
    "        corr = corr.mask(np.abs(corr) < threshold, np.nan)\n",
    "\n",
    "    if upper_tri:\n",
    "        mask = np.tril(np.ones_like(corr, dtype=bool)) # affichera que le triangle supérieur\n",
    "\n",
    "    sns.heatmap(corr, mask=mask, annot = True, fmt = '.2f', cmap = cmap)\n",
    "    plt.show()\n",
    "\n",
    "    return corr\n",
    "\n",
    "# RMSE\n",
    "# @latexify.function\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((targets - predictions) ** 2).mean())\n",
    "\n",
    "# MAPE\n",
    "# @latexify.function\n",
    "def mape(predictions, targets):\n",
    "    return np.mean(np.abs((targets - predictions)) / targets) * 100\n",
    "\n",
    "# MAE\n",
    "# @latexify.function\n",
    "def mae(predictions, targets):\n",
    "    return np.mean(np.abs((targets - predictions)))\n",
    "\n",
    "# Function to check VIF (TOUTES LES FEATURES DOIVENT ETRE EN FLOAT !!)\n",
    "def checking_vif(train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"feature\"] = train.columns\n",
    "\n",
    "    # Calculating VIF for each feature\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n",
    "    ]\n",
    "    return vif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/X_train_Hi5.csv\")\n",
    "df = df.sample(n=10_000, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"piezo_groundwater_level_category\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On tolère 19% de nan dans les colonnes\n",
    "for proportion_nan_prct in [19]:\n",
    "    new_df = cp.deepcopy(df)\n",
    "    for col in new_df.columns:\n",
    "        if new_df[col].isnull().sum() * 100 / len(df) > proportion_nan_prct:\n",
    "            new_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# new_df.info()\n",
    "\n",
    "# Gérer les duplicatas\n",
    "new_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop les features indépendantes qui sont ultra corrélées entre elles (id...)\n",
    "features_to_drop = [\"piezo_station_commune_code_insee\",\n",
    "                    \"piezo_station_pe_label\",\n",
    "                    \"piezo_station_bdlisa_codes\",\n",
    "                    \"piezo_station_bss_code\",\n",
    "                    \"piezo_station_commune_name\",\n",
    "                    \"piezo_station_bss_id\",\n",
    "                    \"piezo_bss_code\",\n",
    "                    \"piezo_station_update_date\",\n",
    "                    \"piezo_qualification\",\n",
    "                    \"piezo_continuity_code\",\n",
    "                    \"piezo_continuity_name\",\n",
    "                    \"piezo_producer_name\",\n",
    "                    \"piezo_measure_nature_name\",\n",
    "                    \"meteo_name\",\n",
    "                    \"hydro_station_code\",\n",
    "                    \"hydro_method_code\",\n",
    "                    \"hydro_method_label\",\n",
    "                    \"insee_med_living_level\",\n",
    "                    \"meteo_id\",\n",
    "                    \"hydro_qualification_label\",\n",
    "                    \"hydro_status_code\",\n",
    "                    \"piezo_station_department_name\"]\n",
    "\n",
    "features = new_df.drop(features_to_drop, axis=1)\n",
    "# features.info()\n",
    "\n",
    "num_col_features =      [\"piezo_station_investigation_depth\", \n",
    "                        \"piezo_station_altitude\", \n",
    "                        \"piezo_station_longitude\", \n",
    "                        \"piezo_station_latitude\", \n",
    "                        \"piezo_producer_code\", \n",
    "                        \"meteo_latitude\", \n",
    "                        \"meteo_longitude\", \n",
    "                        \"meteo_altitude\", \n",
    "                        \"meteo_rain_height\", \n",
    "                        \"meteo_temperature_min\", \n",
    "                        \"meteo_time_tn\", \n",
    "                        \"meteo_temperature_max\", \n",
    "                        \"meteo_time_tx\", \n",
    "                        \"meteo_temperature_avg\", \n",
    "                        \"meteo_temperature_avg_threshold\",\n",
    "                        \"meteo_frost_duration\", \n",
    "                        \"meteo_amplitude_tn_tx\", \n",
    "                        \"meteo_temperature_avg_tntm\", \n",
    "                        \"meteo_evapotranspiration_grid\", \n",
    "                        \"distance_piezo_meteo\", \n",
    "                        \"hydro_observation_result_elab\", \n",
    "                        \"hydro_longitude\",\n",
    "                        \"hydro_latitude\", \n",
    "                        \"distance_piezo_hydro\", \n",
    "                        \"prelev_other_volume_sum\", \n",
    "                        \"insee_%_agri\", \n",
    "                        \"insee_pop_commune\", \n",
    "                        \"insee_%_ind\", \n",
    "                        \"insee_%_const\"]\n",
    "\n",
    "cat_col_features = [\"piezo_station_department_code\",  \n",
    "                    \"piezo_obtention_mode\", \n",
    "                    \"piezo_status\", \n",
    "                    \"piezo_measure_nature_code\", \n",
    "                    \"hydro_status_label\", \n",
    "                    \"hydro_qualification_code\",  \n",
    "                    \"hydro_hydro_quantity_elab\"]\n",
    "\n",
    "\n",
    "# On traite le dataframe features entier\n",
    "new_features = cp.deepcopy(features)\n",
    "\n",
    "new_features[num_col_features] = new_features[num_col_features].replace('N/A - division par 0', np.nan)\n",
    "new_features[num_col_features] = new_features[num_col_features].replace('nan', np.nan)\n",
    "\n",
    "for col in num_col_features:\n",
    "    new_features[col].astype(\"float\")\n",
    "\n",
    "# imputation des valeurs numériques manquantes (np.nan) : \n",
    "\n",
    "imputer_mean_posterior = IterativeImputer(random_state=42, sample_posterior=True)\n",
    "\n",
    "imputer_mean_posterior.fit(new_features[num_col_features])\n",
    "values_imputed = imputer_mean_posterior.transform(new_features[num_col_features])\n",
    "\n",
    "new_features[num_col_features] = values_imputed\n",
    "\n",
    "# # vérif plus de nan en numérique\n",
    "# nan_count = new_features[num_col_features].isna().sum()\n",
    "# print(nan_count)\n",
    "# total_nan_count = new_features[num_col_features].isna().sum().sum()\n",
    "# print(total_nan_count)\n",
    "\n",
    "# Gérer les 3 features datetime\n",
    "\n",
    "new_features['DATE_piezo_measurement_date'] = pd.to_datetime(new_features['piezo_measurement_date'])\n",
    "\n",
    "new_features['year_piezo_measurement'] = new_features['DATE_piezo_measurement_date'].dt.year\n",
    "new_features['month_piezo_measurement'] = new_features['DATE_piezo_measurement_date'].dt.month\n",
    "new_features['day_piezo_measurement'] = new_features['DATE_piezo_measurement_date'].dt.day\n",
    "new_features = new_features.drop(['piezo_measurement_date'], axis=1)\n",
    "\n",
    "new_features['DATE_meteo_date'] = pd.to_datetime(new_features['meteo_date'])\n",
    "\n",
    "new_features['year_meteo_date'] = new_features['DATE_meteo_date'].dt.year\n",
    "new_features['month_meteo_date'] = new_features['DATE_meteo_date'].dt.month\n",
    "new_features['day_meteo_date'] = new_features['DATE_meteo_date'].dt.day\n",
    "new_features = new_features.drop(['meteo_date'], axis=1)\n",
    "\n",
    "\n",
    "new_features['DATE_hydro_observation_date_elab'] = pd.to_datetime(new_features['hydro_observation_date_elab'])\n",
    "\n",
    "new_features['year_hydro_observation_date_elab'] = new_features['DATE_hydro_observation_date_elab'].dt.year\n",
    "new_features['month_hydro_observation_date_elab'] = new_features['DATE_hydro_observation_date_elab'].dt.month\n",
    "new_features['day_hydro_observation_date_elab'] = new_features['DATE_hydro_observation_date_elab'].dt.day\n",
    "new_features = new_features.drop(['hydro_observation_date_elab'], axis=1)\n",
    "\n",
    "\n",
    "new_features_2 = cp.deepcopy(new_features) # au cas où je fais un truc nul\n",
    "\n",
    "\n",
    "# target_feature = new_features_2[target]\n",
    "\n",
    "# Ordinal encoding :\n",
    "\n",
    "ord_cols = [\"piezo_obtention_mode\", \n",
    "            \"piezo_status\", \n",
    "            \"piezo_measure_nature_code\", \n",
    "            \"hydro_status_label\",\n",
    "            \"hydro_qualification_code\",\n",
    "            \"piezo_groundwater_level_category\"]\n",
    "\n",
    "for col in ord_cols:\n",
    "\n",
    "    temp = new_features_2[col]\n",
    "    temp = pd.DataFrame(temp)\n",
    "\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    ordinal_encoder.fit(temp)\n",
    "    ordinal_enc = ordinal_encoder.transform(temp)\n",
    "\n",
    "    new_features_2['ORDINAL_' + col] = ordinal_enc\n",
    "\n",
    "\n",
    "new_features_2 = new_features_2.drop(ord_cols, axis=1)\n",
    "\n",
    "# test.sample(50)\n",
    "\n",
    "# Onehot encoding\n",
    "onehot_cols = [\"piezo_station_department_code\", \"hydro_hydro_quantity_elab\"]\n",
    "new_features_2 = pd.get_dummies(new_features_2, columns=onehot_cols, drop_first=True)\n",
    "# new_features_2[target] = target_feature # tout est onehot encodé sauf la target var\n",
    "\n",
    "# Drop les colonnes avec \"DATE\" (datetime qui sert plus à rien) :\n",
    "\n",
    "date_cols = [\"DATE_piezo_measurement_date\", \"DATE_meteo_date\", \"DATE_hydro_observation_date_elab\"]\n",
    "new_features_2 = new_features_2.drop(date_cols, axis=1)\n",
    "\n",
    "\n",
    "# On cast toutes les features (maintenant toutes encodées donc numérique ou booléen) \n",
    "# en float\n",
    "\n",
    "for col in new_features_2.columns:\n",
    "    new_features_2[col] = new_features_2[col].astype(\"float64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "new_features_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_2.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suite preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virer les nans et inf qu'il y a encore ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = new_features_2.isin([np.inf, -np.inf]) \n",
    "\n",
    "# count = np.isinf(new_features_2).values.sum() \n",
    "# print(\"It contains \" + str(count) + \" infinite values\") \n",
    "\n",
    "new_features_2.isnull().sum().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vire les na qui sont apparus (jsp comment mais pour l'instant on va vite ca vire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_2 = new_features_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_2.isnull().sum().sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop row_index sert à rien à part géner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_2 = new_features_2.drop(\"row_index\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tout doit être en numérique après encoding (pas d'object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(new_features_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cols = [\"piezo_obtention_mode\", \"piezo_status\"]\n",
    "test = new_features_2[ord_cols]\n",
    "test = pd.DataFrame(test)\n",
    "test.head()\n",
    "\n",
    "for col in ord_cols:\n",
    "\n",
    "    temp = test[col]\n",
    "    temp = pd.DataFrame(temp)\n",
    "\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    ordinal_encoder.fit(temp)\n",
    "    ordinal_enc = ordinal_encoder.transform(temp)\n",
    "\n",
    "    test['NUM_' + col] = ordinal_enc\n",
    "\n",
    "test.sample(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ordinal encoding : \n",
    "\n",
    "piezo_obtention_mode\n",
    "1. Valeur mesurée\n",
    "2. Mode d'obtention inconnu\n",
    "3. Valeur reconstituée\n",
    "\n",
    "piezo_status\n",
    "1. Donnée contrôlée niveau 2\n",
    "2. Donnée contrôlée niveau 1\n",
    "3. Donnée brute\n",
    "4. Donnée interprétée\n",
    "\n",
    "piezo_measure_nature_code\n",
    "1. 0\n",
    "2. N\n",
    "3. nan\n",
    "4. I\n",
    "5. D\n",
    "6. S\n",
    "\n",
    "hydro_status_label\n",
    "1. Donnée validée\n",
    "2. Donnée pré-validée\n",
    "3. Donnée brute\n",
    "4. Donnée corrigée\n",
    "\n",
    "hydro_qualification_code\n",
    "1. 20\n",
    "2. 12\n",
    "3. 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "one hot : \n",
    "piezo_station_department_code\n",
    "hydro_hydro_quantity_elab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(new_features[cat_col_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = new_features_2[ord_target].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dependent variable and indepedent variables\n",
    "\n",
    "ord_target = \"ORDINAL_\" + target\n",
    "\n",
    "Y = new_features_2[ord_target]\n",
    "\n",
    "X = new_features_2.drop(columns=ord_target)\n",
    "\n",
    "\n",
    "# Robust Scaling de X\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "transformer = RobustScaler()\n",
    "transformer.set_output(transform=\"pandas\")\n",
    "transformer.fit(X)\n",
    "X_scaled = transformer.transform(X)\n",
    "X_scaled\n",
    "\n",
    "# PCA pour réduire la dimension\n",
    "\n",
    "pca = PCA()\n",
    "pca.set_output(transform=\"pandas\")\n",
    "comp = pca.fit(X_scaled)\n",
    "\n",
    "plt.plot(np.cumsum(comp.explained_variance_ratio_))\n",
    "plt.grid()\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "sns.despine()\n",
    "\n",
    "pca = PCA(n_components=15).fit(X_scaled)\n",
    "X_transformed = pca.transform(X_scaled)\n",
    "\n",
    "X_transformed = pd.DataFrame(X_transformed)\n",
    "\n",
    "X_transformed.to_csv(\"./data/preprocessed_afterPCA_Robust_X_train_10k.csv\")\n",
    "\n",
    "\n",
    "# # Add the intercept term\n",
    "# X_transformed = sm.add_constant(X_transformed)\n",
    "\n",
    "# # splitting the data in 70:30 ratio of train to test data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_transformed, Y, test_size = 0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(Y)\n",
    "\n",
    "for col in b.columns:\n",
    "    b[col] = b[col].astype(\"int\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP pour réduire la dimension\n",
    "\n",
    "import umap.umap_ as umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(X_scaled)\n",
    "embedding.shape\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=[sns.color_palette()[x] for x in b[ord_target].map({\"0\": int(0), \n",
    "                                                \"1\":int(1), \n",
    "                                                \"2\":int(2),\n",
    "                                                \"3\":int(3),\n",
    "                                                \"4\":int(4),\n",
    "                                                })])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Penguin dataset', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = checking_vif(X_train)\n",
    "vif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vire les features VIF > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif[vif[\"VIF\"] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vif = vif[vif[\"VIF\"] < 5][\"feature\"].to_numpy()\n",
    "\n",
    "X_train_2 = X_train[features_vif]\n",
    "X_test_2 = X_test[features_vif]\n",
    "X_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost multiclass classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test naïf rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, \n",
    "                            param_distributions=params, \n",
    "                            random_state=42, \n",
    "                            n_iter=100, \n",
    "                            cv=2, \n",
    "                            verbose=1, \n",
    "                            n_jobs=1, \n",
    "                            return_train_score=True)\n",
    "\n",
    "search.fit(X_train_2, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_best_scores(search.cv_results_, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
