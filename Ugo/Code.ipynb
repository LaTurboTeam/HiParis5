{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nikitaprasad21/ML-Cheat-Codes/blob/main/Feature-Engineering/feature_transformation_basics.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordre pour agir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gérer les outliers (supprimer ou impute ?)\n",
    "2. Imputation\n",
    "3. Encoder les variables\n",
    "4. Feature Transformation\n",
    "5. Feature scaling\n",
    "6. Vectoriser les features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer les données avec trop de skewness par exemple ou pas dans la bonne range... pour que leur distrib soit normale ou autre...\n",
    "\n",
    "Certains modèles tels que : Linear & Logistic Regression nécessitent d'avoir des variables qui suivent une distribution normale.\n",
    "<br>En général, les données du monde réel suivent une normale skewed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Right-skewed data => Log Transformation -> normal data:**\n",
    "*   Attention données négatives\n",
    "\n",
    "**Large values in data => Reciprocal Transformation -> small data for big values:**\n",
    "*   Attention non défini en 0\n",
    "\n",
    "**Left-skewed data => Square Transformation -> normal data:**\n",
    "\n",
    "**Un peu Right-skewed data => Square Root Transformation -> normal data:**\n",
    "*   Attention données négatives\n",
    "*   Effet plus léger sur les data que Log transf.\n",
    "\n",
    "**Large values in data => Box-Cox Transformation -> small data for big values:**\n",
    "*   Attention données > 0 (non défini même en 0)\n",
    "*   Transf. paramétrique général\n",
    "*   Estimation des paramètres optimaux par max de vraisemblance en minimisant la skewness & stabilisant la variance\n",
    "\n",
    "\n",
    "**Distrib à peu près gaussienne mais bizarre => Power Transformations -> normal data:**\n",
    "*   Transf. paramétrique général\n",
    "*   Estimation des paramètres optimaux par max de vraisemblance en minimisant la skewness & stabilisant la variance\n",
    "*   Supporte Box-Cox transformée et la transformée de Yeo-Johnson \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation (Min Max) = Scale les valeurs dans 0 et 1**\n",
    "\n",
    "**Standardisation / Z-score normalisation = centre & réduit les données**\n",
    "\n",
    "**Standardisation robuste (par rapport aux outliers)**\n",
    "\n",
    "https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction & Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On garde un sous groupe de features qui explique la majorité de la variance pour réduire la dimension du problème."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[à compléter y a VIF ...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imbalanced-learn : \n",
    "\n",
    "https://imbalanced-learn.org/stable/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nikitaprasad21/ML-Cheat-Codes/blob/main/Feature-Engineering/imbalanced-data.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ecrire beau rapport latex**\n",
    "\n",
    "* https://github.com/JAEarly/latextable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder = pour données nominales sans ordre** (chat chien renard)\n",
    "\n",
    "**OrdinalEncoder = pour données nominales avec ordre** (mauvais, bon, tres bien)\n",
    "\n",
    "**LabelEncoder = pour les labels**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour les avoir des références à des articles scientifiques**\n",
    "\n",
    "* Medium\n",
    "    * Il y a des références à la fin des articles Medium parfois, que je pourrais ajouter lors du concours\n",
    "\n",
    "\n",
    "**Penser à regarder les hypothèses des modèles de prédiction**\n",
    "\n",
    "* naïve bayes par exemple : indépendance des prédicteurs\n",
    "\n",
    "\n",
    "**Log la target variable qu'on doit prédire ?**\n",
    "\n",
    "La variable cible est la \"variable dépendante\".\n",
    "<br> Si on choisit la régression linéaire, une variable dépendante non normale (non gaussienne) produira \n",
    "<br> des résidus non normaux. \n",
    "\n",
    "Solution \n",
    "<br> -> Pour rendre normale notre variable (un peu skewed par exemple) : log(target) & vérifier la nouvelle distribution\n",
    "\n",
    "p. 171 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lorsqu'on onehot encode : toujours drop 1 colonne pour éviter de créer une multicolinéarité**\n",
    "\n",
    "p. 27 livre datascience"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ne pas onehot encode pour n'importe quel modèle !**\n",
    "\n",
    "p.31 livre datascience"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si bcp de features, remplacer LinearRegression() de scikit par SGDRegressor()**\n",
    "\n",
    "p. 49 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA tips**\n",
    "\n",
    "p. 61 livre ds\n",
    "\n",
    "p. 76 livre ds\n",
    "\n",
    "p. 108 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quels modèles nécessitent de normaliser / scale ses données**\n",
    "\n",
    "p. 66 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Faire une webapp rapidement**\n",
    "\n",
    "p. 90 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partial training sur gros datasets**\n",
    "\n",
    "p. 94 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficients de corrélation**\n",
    "\n",
    "p. 111 livre ds\n",
    "\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC6107969/\n",
    "\n",
    "paper new ksi coefficient\n",
    "\n",
    "*   relation forte entre 2 variables\n",
    "*   corrélation périodique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramètre de régularisation - granularité**\n",
    "\n",
    "p. 133 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ne pas overfit un arbre de décision grâce au Sankey Diagram**\n",
    "\n",
    "p. 142 livre ds\n",
    "\n",
    "p. 170 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problème scatter plot : peu y avoir du clutter - fouilli**\n",
    "\n",
    "p. 146 livre ds\n",
    "\n",
    "p. 237 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle les données pour créer les train/test split**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plutôt que faire des barplots pour illustrer un propos : bubble charts**\n",
    "\n",
    "p. 188 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dès qu'une hypothèse n'est pas vérifiée régression linéaire : chercher une solution**\n",
    "\n",
    "Exemple pour hétéroscédasticité avec reg. linéaire : \n",
    "\n",
    "https://statisticsbyjim.com/regression/heteroscedasticity-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preuve que violinplot / (histplot+kde) >> boxplot**\n",
    "\n",
    "Ne pas utiliser boxplot !!\n",
    "\n",
    "p. 192 livre ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outils Data Scientist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FireDucks x27 faster vs Pandas** (dispo sous linux uniquement)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**icecream : debug fast variables**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations techniques/Bonnes pratiques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Est ce qu'on est en supervisé ou non supervisé ?**\n",
    "    * Modèles pour le supervisé\n",
    "        * Problème de Classification\n",
    "            * Regression logistique     # régression linéaire généralisé pour la classification\n",
    "            * Arbre de décision         : utiliser dtreeviz pour générer un graphique\n",
    "            * Random Forest\n",
    "            * Naïve Bayes               # Suppose indépendance entre les prédicteurs\n",
    "            * SVM\n",
    "            * DNN\n",
    "\n",
    "        * Problème de Régression \n",
    "            * Régression linéaire     \n",
    "            * Régression polynomiale\n",
    "            * ... (voir p. 82 livre datascience)\n",
    "            * \n",
    "\n",
    "\n",
    "    * Modèles pour le non supervisé\n",
    "        * Problème de groupage - clustering\n",
    "            * KMeans++ ou Breathing KMeans (meilleur algo que KMeans simple)\n",
    "            * DBSCAN    mieux que KMeans (basé sur distrib alors que KMeans fonctionne sur clusters sphériques)\n",
    "            * ... (voir p. 42 | 103 livre datascience)\n",
    "\n",
    "\n",
    "        * Problème d'association \n",
    "        * Autoencodeurs\n",
    "            * Missing Value imputation \n",
    "            * Dimensionality Reduction\n",
    "            * Feature Extraction \n",
    "\n",
    "*   **Commencer par construire un modèle linéaire**\n",
    "    * car il faut toujours essayer de voir si le modèle le plus simple fonctionne\n",
    "    * et ensuite compléxifier si besoin\n",
    "        * XGBoostRegressor\n",
    "        * K-means\n",
    "        * SVM\n",
    "        * Réseau de neurones\n",
    "\n",
    "*   **Eviter l'underfitting**\n",
    "    * Feature selection/engineering - prendre les bonnes features\n",
    "    * Si le modèle n'est pas suffisant pour décrire la complexité des données\n",
    "        -> compléxifier le modèle\n",
    "\n",
    "*   **Eviter l'overfitting**\n",
    "    * Feature selection/engineering - prendre les bonnes features\n",
    "    * ne pas overtrain le modèle\n",
    "    * Réduction de dimension/features (PCA, VIF, régularisation...)\n",
    "    * Méthode d'ensemble learning (Stacking, bagging, boosting)\n",
    "    * Vérifier les résultats obtenus pour les Cross Validation\n",
    "        * si y a 3 plis et que le modèle fait : [13%, 78%, 47%] -> il n'arrive pas à généraliser correctement\n",
    "        * si y a 3 plis et que le modèle fait : [78%, 71%, 83%] -> on prend le modèle qui a obtenu les meilleurs résultats (83% ici)\n",
    "        * En général, K=10 plis c'est bien\n",
    "    \n",
    "*   **Evaluer les performances du modèle**\n",
    "    * Explained Variance Score & R2\n",
    "        * https://stats.stackexchange.com/questions/210168/what-is-the-difference-between-r2-and-variance-score-in-scikit-learn\n",
    "    * MSE\n",
    "    * Métriques propres au cas d'utilisation du modèle (si classification : AUC ROC ou AUC PR...)\n",
    "\n",
    "    * Weighted Accuracy pour de la classification : https://store.ectap.ro/articole/1421.pdf  (p. 6 en bas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputation des données manquantes**\n",
    "\n",
    "*   La meilleure méthode pour données MAR est sampling de posterior (d'après mon livre Theory Decision Making) : \n",
    "<br> https://scikit-learn.org/1.5/modules/impute.html (chercher sur la page : sample_posterior=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarder ces notebooks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/vikrishnan/boston-house-prices/code?datasetId=1815&sortBy=voteCount"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebooks pour comprendre le feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NishadKhudabux/Boston-House-Price-Prediction/blob/main/Boston_HousePrice_Prediction_Regression.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/ohseokkim/linear-nonlinear-scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries for data manipulation\n",
    "import pandas as pd\n",
    "from summarytools import dfSummary\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from icecream import ic\n",
    "#ic(my_var) : fait un joli print direct\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# import latexify\n",
    "\n",
    "# Import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import copy as cp\n",
    "\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "\n",
    "# Import libraries for building linear regression model\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Import the required function\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import library for preparing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import library for data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# Plot q-q plot of residuals\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "\n",
    "## R2\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corr_plot(df, upper_tri=None, threshold=None):\n",
    "    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
    "\n",
    "    corr = df.corr()\n",
    "    mask = None\n",
    "\n",
    "    if threshold != None:\n",
    "        corr = corr.mask(np.abs(corr) < threshold, np.nan)\n",
    "\n",
    "    if upper_tri:\n",
    "        mask = np.tril(np.ones_like(corr, dtype=bool)) # affichera que le triangle supérieur\n",
    "\n",
    "    sns.heatmap(corr, mask=mask, annot = True, fmt = '.2f', cmap = cmap)\n",
    "    plt.show()\n",
    "\n",
    "    return corr\n",
    "\n",
    "\n",
    "\n",
    "# RMSE\n",
    "# @latexify.function\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((targets - predictions) ** 2).mean())\n",
    "\n",
    "# MAPE\n",
    "# @latexify.function\n",
    "def mape(predictions, targets):\n",
    "    return np.mean(np.abs((targets - predictions)) / targets) * 100\n",
    "\n",
    "# MAE\n",
    "# @latexify.function\n",
    "def mae(predictions, targets):\n",
    "    return np.mean(np.abs((targets - predictions)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Régression linéaire**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NishadKhudabux/Boston-House-Price-Prediction/blob/main/Boston_HousePrice_Prediction_Regression.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do : \n",
    "\n",
    "*   **Charger le dataset et regarder quelques lignes**  <span style=\"color:red\">Spark</span>\n",
    "*   **Rendre exploitable le dataset** :\n",
    "    - cast une colonne string en int/float ou autre type plus adapté\n",
    "    - convertir les dates string en datetime\n",
    "    - extraire les colonnes weekday, month, year de la colonne date dd-mm-yyyy (cela permettra plus tard de discerner des tendances dans la semaine/ le mois/ les années)\n",
    "    - ...\n",
    "*   **Vérifier les dtype des colonnes** (checker qu'elles ont toute le bon type)\n",
    "*   **Checker le nombre de ligne unique par feature/colonne** (permet de checker suivant le dataset si les données sont cohérentes, parfois ok si duplicata d'info, parfois c'est pas normal)\n",
    "*   **Nombre de feature catégorielle et numérique**\n",
    "*   **Regarder les statistiques basiques pour chaque feature** (vérifier que les valeurs moyennes, std max min pas incohérent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**\n",
    "\n",
    "\n",
    "[Décrire ce que chaque ligne représente]. \n",
    "<br> D'où viennent les données (source, date, contexte).\n",
    "\n",
    "Attribute informations : \n",
    "\n",
    "[expliquer chaque colonne]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/X_train_Hi5.csv\")\n",
    "df = df.sample(n=10_000, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse univariée (sur chaque colonne)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do**\n",
    "\n",
    "[Faire des observations à l'oeil nu et suppositions sur chaque colonne]\n",
    "<br> Exemple : \n",
    "\n",
    "*   quantile 75% vaut 3.6 mais le max est à 88 : il y a des outliers\n",
    "*   0.38-0.87 with a average of 0.55. Distribution looks normal.\n",
    "*   Colonne qui a 75th percentile à la valeur max : suggère qu'il y a 2 types d'habitation dans notre cas d'étude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :**\n",
    "\n",
    "CRIM : \n",
    "*   right skewness avec la plupart des valeurs à 0\n",
    "\n",
    "ZN : \n",
    "*   Approche distribution uniforme\n",
    "\n",
    "[Déduire des infos à propos des maisons présentes dans ce dataset]\n",
    "\n",
    "MEDV : \n",
    "*   C'est la variable target - dépendante.\n",
    "<br> On constate qu'elle n'est pas normale (un peu skewed).\n",
    "\n",
    "En la passant au log : elle devient normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MEDV_log'] = np.log(df['MEDV'])\n",
    "sns.histplot(data = df, x = 'MEDV_log', kde = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse bivariée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = corr_plot(df, upper_tri=True, threshold=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle, Rectangle\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_plot(df, upper_tri=True, threshold=0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Observations sur correlations avec la target var]\n",
    "\n",
    "*   [var qui a la plus grosse correl positive avec target]\n",
    "*   [var qui a la plus grosse correl négative avec target]\n",
    "\n",
    "*   [noter var qui ont une correl linéaire négative avec target : features indésirables dans notre modèle linéaire]\n",
    "\n",
    "--------------------------------------------\n",
    "[Correlations FORTES |Cov(x,y)| >= 0.7 des variables indépendantes (tout sauf target)]\n",
    "\n",
    "*   [noter ces var]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####\n",
    "*   [observer la relation entre ces features]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(AGE,DIS)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   une décroissance linéaire avec DIS en fonction de AGE.\n",
    "\n",
    "->RAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'AGE', y = 'DIS', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(RAD,TAX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   Très grande corrélation MAIS aucune trend visible\n",
    "\n",
    "-> Forte corrélation possiblement dû aux outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'RAD', y = 'TAX', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification après avoir enlevé les outliers : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the data corresponding to high tax rate\n",
    "df1 = df[df['TAX'] < 600]\n",
    "\n",
    "# Calculate the correlation\n",
    "print('The correlation between TAX and RAD is', pearsonr(df1['TAX'], df1['RAD'])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(INDUS,TAX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   TAX semble croître linéairement en fonction de INDUS\n",
    "\n",
    "-> RAS mais bizarre en soi que ces 2 variables soient liées : dépendance avec une 3ème variable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'INDUS', y = 'TAX', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(RM,MEDV)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   une croissance linéaire de MEDV en fonction de RM.\n",
    "*   La valeur de MEDV semble cappée à 50\n",
    "\n",
    "->RAS & présence d'outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'RM', y = 'MEDV', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(LSTAT,MEDV)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   une décroissance à peu près linéaire de MEDV en fonction de LSTAT.\n",
    "*   La valeur de MEDV semble cappée à 50\n",
    "\n",
    "->RAS & présence d'outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'LSTAT', y = 'MEDV', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(INDUS,NOX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   une croissance linéaire de NOX en fonction de INDUS.\n",
    "*   pas d'outlier évident\n",
    "\n",
    "->RAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'INDUS', y = 'NOX', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(AGE,NOX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   une croissance linéaire de NOX en fonction de AGE.\n",
    "*   groupe de valeurs haute de NOX possiblement outliers\n",
    "\n",
    "->RAS & outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'AGE', y = 'NOX', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "**(DIS,NOX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate : \n",
    "\n",
    "*   une décroissance linéaire forte de NOX en fonction de DIS.\n",
    "\n",
    "->RAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to visualize the relationship between AGE and DIS\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.scatterplot(x = 'DIS', y = 'NOX', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####\n",
    "*   [noter les variables indépendantes à éliminer (possiblement dans le training set ensuite PAS MAINTENANT)]\n",
    "\n",
    "    -> quand on a une variable x : Cov(x,target) forte c'est bien \n",
    "    <br> mais \n",
    "    <br> si on a 2 variables x,y != target, avec une forte dépendance Cov(x,y), \n",
    "    <br> elles n'apporteront pas plus d'explication l'une par rapport à l'autre \n",
    "    <br> sur notre variable target puisqu'elle apportent à peu près la même info.\n",
    "    <br> (donc n'en garder qu'une des deux)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do :\n",
    "Note : \n",
    "\n",
    "<span style=\"color:red\">Le terme \"Gérer\" ne veut pas toujours dire \"supprimer\" !!</span>\n",
    "<br>Exemple : pour les NaN et même si on veut pour les outliers, il vaut mieux faire du missing input avec une technique qui convient : posterior sampling\n",
    "\n",
    "*   **Gérer les lignes dupliquées (entièrement)**\n",
    "*   **Gérer les valeurs Null, vides** \n",
    "*   **Gérer les valeurs vides - avec espace** \n",
    "*   **Gérer les outliers**\n",
    "*   **Gérer les autres valeurs (\"Infinity\", +np.inf(), -np.inf(), autres...)**\n",
    "*   **SI BESOIN - Convertir les colonnes catégorielles en numérique (pas toujours nécessaire suivant le modèle)** \n",
    "    *   (one hot encoding, ...)\n",
    "*   **Comparer la taille du dataset preprocessed et brut** (pour vérifier qu'on a pas trop enlevé de données)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virer feature nan count => alpha%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proportion_nan_prct à trouver graphiquement car parfois baisser le seuil de nan n'impacte pas le nb de features conservées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_nb_col_remaining = []\n",
    "arr_proportion_nan_prct = np.arange(100)\n",
    "for proportion_nan_prct in arr_proportion_nan_prct:\n",
    "    new_df = cp.deepcopy(df)\n",
    "    for col in new_df.columns:\n",
    "        if new_df[col].isnull().sum() * 100 / len(df) > proportion_nan_prct:\n",
    "            new_df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    nb_col_remaining = new_df.shape[1]\n",
    "    arr_nb_col_remaining.append(nb_col_remaining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arr_proportion_nan_prct, arr_nb_col_remaining)\n",
    "plt.xlabel(\"proportion_nan_prct toléré\")\n",
    "plt.ylabel(\"Nb features qui restent\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut faire : \n",
    "\n",
    "*   1er temps : modèle qui a proportion_nan_prct = 19\n",
    "*   2ème temps : modèle qui a proportion_nan_prct = 58 (pour conserver plus de features et améliorer possiblement l'explication de la variance des données)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for proportion_nan_prct in [19]:\n",
    "    new_df = cp.deepcopy(df)\n",
    "    for col in new_df.columns:\n",
    "        if new_df[col].isnull().sum() * 100 / len(df) > proportion_nan_prct:\n",
    "            new_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "one hot encoding\n",
    "------------------\n",
    "\n",
    "piezo_station_department_code\n",
    "piezo_station_department_name\n",
    "piezo_obtention_mode\n",
    "piezo_status\n",
    "piezo_measure_nature_code\n",
    "hydro_status_code\n",
    "hydro_status_label\n",
    "hydro_qualification_code\n",
    "hydro_hydro_quantity_elab\n",
    "\n",
    "\n",
    "\n",
    "ordinal encoding \n",
    "------------------\n",
    "hydro_qualification_label (douteuse, non qualifiée, bonne)\n",
    "\n",
    "\n",
    "\n",
    "décomposer la feature en numérique: \n",
    "------------------\n",
    "piezo_measurement_date (décomposé day, month, year)\n",
    "meteo_date (décomposé day, month, year)\n",
    "hydro_observation_date_elab\n",
    "\n",
    "drop feature: \n",
    "------------------\n",
    "piezo_station_commune_code_insee\n",
    "piezo_station_pe_label\n",
    "piezo_station_bdlisa_codes\n",
    "piezo_station_bss_code\n",
    "piezo_station_commune_name\n",
    "piezo_station_bss_id\n",
    "piezo_bss_code\n",
    "piezo_station_update_date (décomposé hour, day, month)\n",
    "piezo_qualification\n",
    "piezo_continuity_code\n",
    "piezo_continuity_name\n",
    "piezo_producer_name\n",
    "piezo_measure_nature_name\n",
    "meteo_name\n",
    "hydro_station_code\n",
    "hydro_method_code\n",
    "hydro_method_label\n",
    "insee_med_living_level\n",
    "\n",
    "parse : \n",
    "------------------\n",
    "insee_%_agri float\n",
    "insee_%_ind\n",
    "insee_%_const\n",
    "\n",
    "\n",
    "target : \n",
    "------------------\n",
    "piezo_groundwater_level_category\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(new_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gérer les lignes dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df.shape)\n",
    "new_df.drop_duplicates(inplace=True)\n",
    "print(new_df.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virer les multicolinéarités"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que c'est encodé on peut encore réduire les dimensions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nb var catégorielle/numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_df.columns:\n",
    "    if is_numeric_dtype(new_df[col]):\n",
    "        print(col)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur de l'intercept peut être interprétée comme étant la valeur en espérance de la variable target \n",
    "<br> quand toutes les variables indépendantes sont mises à 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dependent variable and indepedent variables\n",
    "Y = df['MEDV_log']\n",
    "\n",
    "X = df.drop(columns = {'MEDV', 'MEDV_log'})\n",
    "\n",
    "# Add the intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# splitting the data in 70:30 ratio of train to test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection/Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "\n",
    "https://medium.com/@satyarepala/tackling-multicollinearity-understanding-variance-inflation-factor-vif-and-mitigation-techniques-2521ebf024b6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gérer les multicolinéarités**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforme les features en features orthogonales entre elles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VIF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Selection**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check VIF\n",
    "def checking_vif(train):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"feature\"] = train.columns\n",
    "\n",
    "    # Calculating VIF for each feature\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n",
    "    ]\n",
    "    return vif\n",
    "\n",
    "\n",
    "checking_vif(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[enlever les variables muticolinéaires & VERIFIER que les multicolinéarités ont disparues]\n",
    "\n",
    "*   VIF > 5 : éliminé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model after dropping TAX\n",
    "X_train = X_train.drop(columns = 'TAX')\n",
    "\n",
    "# Check for VIF\n",
    "print(checking_vif(X_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Régularisation** - Techniques intégrées dans la régression linéaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une : \n",
    "\n",
    "*   Régression linéaire lasso   L1\n",
    "*   Régression linéaire ridge   L2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayer d'utiliser Yellow Brick pour tout ce qui est visualisation des propriétés du modèle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 1\n",
    "\n",
    "R² = 76.9% pas mal mais améliorable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using ordinary least squared\n",
    "model1 = sm.OLS(y_train,X_train).fit()\n",
    "\n",
    "# Get the model summary\n",
    "model1.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modèle 2\n",
    "\n",
    "drop: ZN | AGE | INDUS\n",
    "\n",
    "R² = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le modèle 1 : \n",
    "\n",
    "les coefficients suivants ne sont pas statistiquement significativement différents de 0 (seuil de confiance 95%) pour cette population :\n",
    "\n",
    "*   P(ZN>|t|) = 0.155\n",
    "*   P(AGE>|t|) = 0.645\n",
    "*   P(INDUS>|t|) = 0.883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model after dropping columns 'MEDV', 'MEDV_log', 'TAX', 'ZN', 'AGE', 'INDUS' from df DataFrame\n",
    "Y = df['MEDV_log']\n",
    "\n",
    "X = df.drop(['ZN','AGE','INDUS'], axis=1)\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Splitting the data in 70:30 ratio of train to test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30 , random_state = 1)\n",
    "\n",
    "# Create the model\n",
    "model2 = sm.OLS(y_train,X_train).fit()\n",
    "\n",
    "# Get the model summary\n",
    "model2.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vérification des hypothèses du modèle de régression linéaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ces hypothèses ne sont pas vérifiées : \n",
    "\n",
    "*   Indépendance des erreurs (erreurs non corrélées - difficile avec séries temporelles)\n",
    "*   Exogénéité (erreurs centrés)\n",
    "*   Homoscédasticité (erreurs de variance constante)\n",
    "*   Normalité des erreurs (hypothèse plus forte que la 1ère)\n",
    "\n",
    "-> l'estimation du modèle est possible\n",
    "MAIS \n",
    "*   estimateur biaisé\n",
    "&/ou\n",
    "\n",
    "*   estimateur non efficace (à variance non minimale)\n",
    "\n",
    "La normalité des erreurs est non obligatoire mais permet de tirer de bonnes propriétés."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checker sur internet pourquoi ces hypothèses et solution si pas vérifiée**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Mean of residuals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model2.resid\n",
    "print(np.mean(residuals))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Homoscedasticité"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intérêt : \n",
    "\n",
    "Être sûr que les coefficients de la régression sont fiables statistiquement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homoscédasticité = Si les résidus sont distribués de manière symétrique sur la ligne de régression, \n",
    "<br> on dit que les données sont homoscédastiques.\n",
    "\n",
    "Hétéroscédasticité = variance des résidus des features différentes\n",
    "\n",
    "Test Goldfeldquandt avec alpha = 0.05 pour vérifier l'hypothèse :\n",
    "\n",
    "*   Hypothèse nulle : Les résidus sont homoscedastiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"F statistic\", \"p-value\"]\n",
    "test = sms.het_goldfeldquandt(y_train, X_train)\n",
    "result = lzip(name, test)\n",
    "print(result)\n",
    "print(\"p-value = \", round(1 - result[1][1], 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value < 0.05 => On rejete l'hypothèse nulle avec un seuil de confiance de 95%.\n",
    "\n",
    "*   Hétéroscédasticité => hypothèse du modèle non vérifiée\n",
    "*   Modèle moins précis de façon générale \n",
    "*   On peut essayer de résoudre ça en transformant la target variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution si heteroscedastique**\n",
    "\n",
    "https://statisticsbyjim.com/regression/heteroscedasticity-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Linear Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Relation linéaire entre variable dépendante et indépendantes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les features doivent avoir une relation linéaire avec la variable target. \n",
    "\n",
    "Pour vérifier : les résidus ne doivent pas montrer de pattern, ils doivent être distribués aléatoirement et uniformément suivant l'axe x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values\n",
    "fitted = model2.fittedvalues\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "sns.residplot(x = fitted, y = residuals, color = \"lightblue\", lowess = True)\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual PLOT\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Pas de pattern dans résidus vs fitted values\n",
    "\n",
    "=> Hypothèse vérifiée"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Erreurs résiduelles normales (distrib. gaussienne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of residuals\n",
    "\n",
    "sns.histplot(residuals, kde = True)\n",
    "plt.show()\n",
    "\n",
    "stats.probplot(residuals, dist = \"norm\", plot = pylab)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Les résidus sont skewed\n",
    "\n",
    "=> L'hypothèse n'est pas vérifiée : modèle moins précis de façon générale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison des résultats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation simple d'un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance on test and train data\n",
    "def model_pref(olsmodel, x_train, x_test):\n",
    "\n",
    "    # In-sample Prediction\n",
    "    y_pred_train = olsmodel.predict(x_train)\n",
    "    y_observed_train = y_train\n",
    "\n",
    "    # Prediction on test data\n",
    "    y_pred_test = olsmodel.predict(x_test)\n",
    "    y_observed_test = y_test\n",
    "\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Data\": [\"Train\", \"Test\"],\n",
    "            \"RMSE\": [\n",
    "                rmse(y_pred_train, y_observed_train),\n",
    "                rmse(y_pred_test, y_observed_test),\n",
    "            ],\n",
    "            \"MAE\": [\n",
    "                mae(y_pred_train, y_observed_train),\n",
    "                mae(y_pred_test, y_observed_test),\n",
    "            ],\n",
    "            \"MAPE\": [\n",
    "                mape(y_pred_train, y_observed_train),\n",
    "                mape(y_pred_test, y_observed_test),\n",
    "            ],\n",
    "            \"r2\": [\n",
    "                r2_score(y_pred_train, y_observed_train),\n",
    "                r2_score(y_pred_test, y_observed_test),\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Checking model performance\n",
    "model_pref(model2, X_train, X_test)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Train & Test scores sont proches et bons, donc le modèle n'est pas overfitté et généralise bien.\n",
    "*   Les scores train & test sont si proches que les chances d'améliorer encore les performances de ce modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-fold Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "\n",
    "\n",
    "# Build the regression model and cross-validate\n",
    "linearregression = LinearRegression()                                    \n",
    "\n",
    "cv_Score11 = cross_val_score(linearregression, X_train, y_train, cv = 10)\n",
    "cv_Score12 = cross_val_score(linearregression, X_train, y_train, cv = 10, \n",
    "                             scoring = 'neg_mean_squared_error')                                  \n",
    "\n",
    "\n",
    "print(\"RSquared: %0.3f (+/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std() * 2))\n",
    "print(\"Mean Squared Error: %0.3f (+/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std() * 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Le modèle retenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = model2.params\n",
    "\n",
    "pd.DataFrame({'Feature' : coef.index, 'Coefs' : coef.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write the equation of the fit\n",
    "\n",
    "Equation = \"log (Price) = \"\n",
    "\n",
    "print(Equation, end = '\\t')\n",
    "\n",
    "for i in range(len(coef)):\n",
    "    print('(', coef[i], ') * ', coef.index[i], '+', end = ' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Weighted Linear Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression linéaire généralisée au cas où l'hypothèse d'homoscedasticité n'est pas vérifiée\n",
    "\n",
    "https://towardsdatascience.com/weighted-linear-regression-2ef23b12a6d7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Régression polynomiale**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Régression DNN**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utiliser LazyPredict pour déblayer le passage**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/suneelpatel/compare-ml-models-with-few-lines-of-code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
